{"id":"atlas-0j7","title":"Integration test: Multiple iteration challenger loop","description":"Create integration test for the re-challenge loop (Phase F).\n\n**Test Setup**:\n- COBOL program\n- Mock challenger that raises issues initially\n- Issues resolved after first round but new issues found\n- Terminates after configured max iterations\n\n**Test Flow**:\n1. Initial documentation produced\n2. First challenge: raises issues\n3. Follow-ups and patch merge\n4. Second challenge: new minor issues\n5. Second round of follow-ups\n6. Third challenge: no blocking issues\n7. Job completes\n\n**Iteration Tracking**:\n- Verify cycle_number increments\n- Artifacts stored at cycle-specific paths\n- Each cycle's state preserved\n\n**Termination Conditions**:\n- Test max_iterations limit\n- Test acceptable_severity threshold\n- Test all issues resolved path\n\n**Assertions**:\n- Loop terminates correctly\n- All cycles have artifacts\n- Final doc includes all improvements\n- Termination reason recorded","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:14:55.495616011-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:28.03138323-07:00","closed_at":"2026-01-15T16:35:28.03138323-07:00","close_reason":"Created multiple iteration challenger loop test verifying follow-up creation, patch merge, and max_iterations enforcement","labels":["challenger","integration-test","testing"],"dependencies":[{"issue_id":"atlas-0j7","depends_on_id":"atlas-4ph","type":"blocks","created_at":"2026-01-15T15:14:55.497234024-07:00","created_by":"andrasfe"},{"issue_id":"atlas-0j7","depends_on_id":"atlas-6mc","type":"blocks","created_at":"2026-01-15T15:14:55.498765356-07:00","created_by":"andrasfe"}]}
{"id":"atlas-0n1","title":"Implement filesystem ArtifactStore adapter","description":"Create a filesystem-based implementation of the ArtifactStore adapter for local development and testing.\n\n**Implementation**:\n- Base directory configuration\n- URI to file path mapping\n- Directory auto-creation\n- Atomic writes (write to temp, rename)\n\n**Features**:\n- JSON/YAML auto-serialization based on extension\n- Checksum verification for content integrity\n- Cycle-aware path generation\n\n**File Organization**:\n```\n{base_path}/\n  {job_id}/\n    manifest.json\n    chunks/\n      {chunk_id}/result.json\n    merges/\n      {merge_node_id}/result.json\n    docs/\n      cycle_{n}/\n        doc.md\n        doc_model.json\n    challenges/\n      cycle_{n}/result.json\n    followups/\n      {issue_id}/answer.json\n```\n\n**Idempotency**:\n- Implement write_if_not_exists using file locking or atomic check","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:09:18.680787552-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:46:14.741777628-07:00","closed_at":"2026-01-15T15:46:14.741777628-07:00","close_reason":"Implemented FilesystemArtifactStore adapter with URI-to-path conversion, async file I/O using asyncio.to_thread, metadata caching, JSON/text convenience methods, SHA-256 content hashing, and comprehensive unit tests (32 tests passing)","labels":["adapter","infrastructure"],"dependencies":[{"issue_id":"atlas-0n1","depends_on_id":"atlas-2y9","type":"blocks","created_at":"2026-01-15T15:09:18.682413771-07:00","created_by":"andrasfe"}]}
{"id":"atlas-0ue","title":"Create sample COBOL test fixtures","description":"Create a library of sample COBOL programs for testing.\n\n**Small Programs** (~100-300 lines):\n- Simple batch program (read file, process, write output)\n- CICS online program with basic screen I/O\n- Program with copybooks\n\n**Medium Programs** (~500-1500 lines):\n- Multiple paragraphs with PERFORM chains\n- FILE-CONTROL with multiple files\n- Error handling patterns (FILE STATUS, ON ERROR)\n- WORKING-STORAGE with various data structures\n\n**Large Programs** (~3000-10000 lines):\n- Complex PROCEDURE DIVISION\n- Multiple sections\n- Nested performs\n- Database access patterns (DB2 or VSAM)\n\n**Edge Cases**:\n- Empty divisions\n- Very long paragraph\n- Deeply nested EVALUATE\n- Complex COPY REPLACING\n- Inline comments variations\n\n**Metadata**:\nEach fixture includes:\n- Source file\n- Expected chunk count (for given budget)\n- Key features to verify (symbols, calls, IO)\n- Expected issues for challenger testing","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:15:17.151344321-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:12:53.994267335-07:00","closed_at":"2026-01-15T16:12:53.994267335-07:00","close_reason":"Created comprehensive COBOL test fixtures: 7 files including small/medium programs, copybooks, and edge cases.","labels":["cobol","fixtures","testing"]}
{"id":"atlas-0ug","title":"Implement job state persistence and recovery","description":"Implement crash recovery and resume capabilities per spec section 12.\n\n**Requirements**:\n- Idempotent and resumable: safe to re-run after crashes/timeouts\n- No duplicate work on restart\n\n**Job State Persistence**:\n- Store current phase\n- Store cycle_number\n- Store mapping: idempotency_key -\u003e work_id\n- Checkpoint after each significant action\n\n**Recovery Algorithm**:\n1. Load job manifest\n2. Load persisted job state\n3. Query all ticket statuses\n4. Compare expected vs actual state\n5. Resume from last consistent point\n6. Re-run reconcile loop\n\n**Ticket Lease Recovery**:\n- Detect stale IN_PROGRESS tickets (lease expired)\n- Reset to READY for re-claiming\n- Track retry counts\n\n**Artifact Recovery**:\n- If ticket DONE but artifact missing -\u003e mark FAILED\n- If artifact exists but ticket not DONE -\u003e verify and update\n\n**Duplicate Prevention**:\n- Check idempotency keys before creating tickets\n- Use manifest map if ticket system lacks uniqueness","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:12:35.418470227-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:03:50.249369361-07:00","closed_at":"2026-01-15T16:03:50.249369361-07:00","close_reason":"Closed","labels":["core","feature","reliability"],"dependencies":[{"issue_id":"atlas-0ug","depends_on_id":"atlas-12g","type":"blocks","created_at":"2026-01-15T15:12:35.420261625-07:00","created_by":"andrasfe"},{"issue_id":"atlas-0ug","depends_on_id":"atlas-yus","type":"blocks","created_at":"2026-01-15T15:12:35.421690194-07:00","created_by":"andrasfe"}]}
{"id":"atlas-0va","title":"Implement Phase A: Request and Plan handler","description":"Implement the Request \u0026 Plan phase per spec section 8 Phase A.\n\n**Trigger**: DOC_REQUEST ticket is created\n\n**Actions**:\n1. Parse DOC_REQUEST payload (artifact_ref, profiles, budget)\n2. Fetch source artifact\n3. Run chunk splitter to produce chunk plan\n4. Run merge DAG planner\n5. Create manifest with chunks[], merge_dag[], review_policy\n6. Write manifest to artifact store\n7. Create DOC_CHUNK tickets for each chunk (status: READY)\n8. Create DOC_MERGE tickets for each merge node (status: BLOCKED)\n9. Update DOC_REQUEST with manifest_uri\n10. Transition DOC_REQUEST to DONE\n\n**Output**:\n- Manifest stored at manifest_uri\n- N chunk tickets (READY)\n- M merge tickets (BLOCKED with dependency edges)\n\n**Optional DOC_PLAN**:\n- If configured, create DOC_PLAN ticket instead of inline planning\n- DOC_PLAN handler does steps 2-10","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:10:16.132426491-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:12:53.7520807-07:00","closed_at":"2026-01-15T16:12:53.7520807-07:00","close_reason":"Phase A handler implemented. Handles DOC_REQUEST work items, coordinates with Planner to generate chunk/merge work items.","labels":["controller","feature","phase-a"],"dependencies":[{"issue_id":"atlas-0va","depends_on_id":"atlas-12g","type":"blocks","created_at":"2026-01-15T15:10:16.134178827-07:00","created_by":"andrasfe"},{"issue_id":"atlas-0va","depends_on_id":"atlas-d7c","type":"blocks","created_at":"2026-01-15T15:10:16.135484245-07:00","created_by":"andrasfe"},{"issue_id":"atlas-0va","depends_on_id":"atlas-ekf","type":"blocks","created_at":"2026-01-15T15:10:16.136709041-07:00","created_by":"andrasfe"}]}
{"id":"atlas-12g","title":"Implement Controller reconcile loop core","description":"Implement the core controller reconcile loop per spec sections 2 and 8.\n\n**Design Principle** (section 2):\nController operates as: observe state -\u003e compute missing work -\u003e create it -\u003e gate/advance\n\n**Reconcile Loop**:\n1. Load job manifest\n2. Query current ticket states\n3. Compute expected state vs actual\n4. Create missing tickets (chunk, merge, challenge, followup)\n5. Update blocked tickets to ready when dependencies complete\n6. Handle completed work (advance to next phase)\n\n**State Machine Phases** (section 8):\n- Phase A: Request \u0026 Plan\n- Phase B: Chunk Analysis\n- Phase C: Hierarchical Merge\n- Phase D: Challenger Review\n- Phase E: Follow-up Dispatch \u0026 Patch Merge\n- Phase F: Re-challenge Loop\n\n**Interface**:\n- reconcile(job_id) -\u003e ReconcileResult\n- ReconcileResult includes:\n  - tickets_created\n  - tickets_transitioned\n  - phase_advanced\n  - job_complete\n\n**Idempotency**:\n- Reconcile is safe to call repeatedly\n- Uses idempotency keys to prevent duplicate tickets","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:10:06.350687347-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:49:11.021300867-07:00","closed_at":"2026-01-15T15:49:11.021300867-07:00","close_reason":"Implemented ReconcileController with reconcile loop, job initialization, phase detection, blocked item advancement, challenger issue routing, and patch merge creation. All 14 reconciler tests pass.","labels":["controller","core","feature"],"dependencies":[{"issue_id":"atlas-12g","depends_on_id":"atlas-fu3","type":"blocks","created_at":"2026-01-15T15:10:06.352405268-07:00","created_by":"andrasfe"},{"issue_id":"atlas-12g","depends_on_id":"atlas-2y9","type":"blocks","created_at":"2026-01-15T15:10:06.353679087-07:00","created_by":"andrasfe"},{"issue_id":"atlas-12g","depends_on_id":"atlas-yus","type":"blocks","created_at":"2026-01-15T15:10:06.354891059-07:00","created_by":"andrasfe"},{"issue_id":"atlas-12g","depends_on_id":"atlas-dxu","type":"blocks","created_at":"2026-01-15T15:10:06.356120274-07:00","created_by":"andrasfe"}]}
{"id":"atlas-1y6","title":"Implement Phase C: Hierarchical merge coordination","description":"Implement Phase C merge coordination per spec section 8 Phase C.\n\n**Phase C Flow**:\n1. When all inputs for a merge node are DONE, controller transitions merge to READY\n2. Aggregator claims merge ticket\n3. Aggregator reads input artifacts, produces merge result\n4. Aggregator marks ticket DONE\n5. Controller checks if parent merges can now be unblocked\n6. Repeat until root merge completes\n\n**Root Merge Special Handling**:\nWhen root merge completes:\n1. Root merge produces doc_uri and doc_model_uri\n2. Controller creates DOC_CHALLENGE ticket (READY)\n3. Advance to Phase D\n\n**Bottom-Up Coordination**:\n- Merges execute in topological order (leaves -\u003e root)\n- Each level waits for all child nodes\n\n**Merge Result Validation**:\n- Verify output artifact exists and is valid\n- Check coverage (all inputs included)\n- Log any conflicts detected","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:10:34.795775177-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:34:59.56284965-07:00","closed_at":"2026-01-15T16:34:59.56284965-07:00","close_reason":"Implemented Phase C: Hierarchical merge coordination. The _advance_merge_items method now processes merge nodes level-by-level (bottom-up) to ensure proper hierarchical advancement. Added logging for phase C events and root merge completion detection.","labels":["controller","feature","phase-c"],"dependencies":[{"issue_id":"atlas-1y6","depends_on_id":"atlas-c19","type":"blocks","created_at":"2026-01-15T15:10:34.79760082-07:00","created_by":"andrasfe"}]}
{"id":"atlas-1yb","title":"Implement Scribe worker interface and base handler","description":"Create the Scribe (chunk analyst) worker interface per spec section 3.2.\n\n**Scribe Responsibilities**:\n- Claim DOC_CHUNK tickets\n- Read source slice per chunk_locator\n- Analyze chunk and produce structured Chunk Result artifact\n- Record open_questions if context insufficient\n- Write result to result_uri\n- Mark ticket DONE\n\n**Worker Interface**:\n```python\nclass ScribeWorker:\n    def claim_next_chunk(self) -\u003e Optional[WorkItem]\n    def process_chunk(self, work_item: WorkItem) -\u003e ChunkResult\n    def complete_chunk(self, work_id, result: ChunkResult)\n```\n\n**Analysis Requirements** (section 7.1):\nProduce ChunkResult with:\n- summary (narrative)\n- facts (symbols, calls, io_operations, error_handling)\n- evidence (line ranges)\n- open_questions (explicit unknowns)\n- confidence\n\n**LLM Integration Points**:\n- Abstract LLM call interface for analysis\n- Prompt template for chunk analysis\n- Structured output parsing\n\n**Idempotency** (section 12.2):\nIf result_uri already exists and is valid, mark DONE without recomputation","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:11:26.083868435-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:49:11.714367171-07:00","closed_at":"2026-01-15T15:49:11.714367171-07:00","close_reason":"Implemented ScribeWorker with LLM-based chunk analysis extracting symbols, calls, I/O operations, error handling patterns. Includes followup answer capability. All worker tests pass.","labels":["feature","scribe","worker"],"dependencies":[{"issue_id":"atlas-1yb","depends_on_id":"atlas-fu3","type":"blocks","created_at":"2026-01-15T15:11:26.085839921-07:00","created_by":"andrasfe"},{"issue_id":"atlas-1yb","depends_on_id":"atlas-2y9","type":"blocks","created_at":"2026-01-15T15:11:26.08718342-07:00","created_by":"andrasfe"},{"issue_id":"atlas-1yb","depends_on_id":"atlas-p00","type":"blocks","created_at":"2026-01-15T15:11:26.088462148-07:00","created_by":"andrasfe"}]}
{"id":"atlas-222","title":"Implement Challenger worker interface and review handler","description":"Create the Challenger (reviewer) worker interface per spec section 3.4.\n\n**Challenger Responsibilities**:\n- Claim DOC_CHALLENGE tickets\n- Read final/near-final merged documentation\n- Identify unclear/incomplete/inconsistent content\n- Produce questions/issues\n- Propose resolution plan identifying follow-up scopes\n\n**Worker Interface**:\n```python\nclass ChallengerWorker:\n    def claim_challenge(self) -\u003e Optional[WorkItem]\n    def review_documentation(self, work_item: WorkItem) -\u003e ChallengeResult\n    def complete_challenge(self, work_id, result: ChallengeResult)\n```\n\n**Challenge Requirements** (section 7.4):\nProduce ChallengeResult with:\n- issues[] each with:\n  - issue_id\n  - severity (BLOCKER|MAJOR|MINOR|QUESTION)\n  - question/problem_statement\n  - doc_section_refs[]\n  - suspected_scopes[]\n  - routing_hints\n- resolution_plan with recommended follow-up tasks\n\n**Challenge Profile**:\n- What to look for: error handling, I/O, restartability, etc.\n- Severity thresholds\n- Focus areas\n\n**Routing Hints Generation**:\nChallenger should provide actionable routing hints:\n- Symbol names referenced\n- Paragraph names\n- File references\n- Chunk IDs if known","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:11:45.95196229-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:49:13.99237216-07:00","closed_at":"2026-01-15T15:49:13.99237216-07:00","close_reason":"Implemented ChallengerWorker with profile-based documentation review, issue identification with severity levels, routing hints extraction, and resolution plan creation. All worker tests pass.","labels":["challenger","feature","worker"],"dependencies":[{"issue_id":"atlas-222","depends_on_id":"atlas-fu3","type":"blocks","created_at":"2026-01-15T15:11:45.953790248-07:00","created_by":"andrasfe"},{"issue_id":"atlas-222","depends_on_id":"atlas-2y9","type":"blocks","created_at":"2026-01-15T15:11:45.95511966-07:00","created_by":"andrasfe"},{"issue_id":"atlas-222","depends_on_id":"atlas-p00","type":"blocks","created_at":"2026-01-15T15:11:45.956407114-07:00","created_by":"andrasfe"}]}
{"id":"atlas-2ks","title":"Integration test: Large COBOL program with hierarchical merges","description":"Create integration test for a large COBOL program per acceptance test scenario 1.\n\n**Scenario** (spec section 15):\nHuge COBOL file with deterministic chunks and hierarchical merges.\n\n**Test Setup**:\n- Large COBOL program (~5000 lines)\n- Forces multi-level merge DAG\n- Mock LLM with chunked responses\n\n**Test Flow**:\n1. Create DOC_REQUEST\n2. Controller creates manifest with many chunks\n3. Process chunks in parallel (simulated)\n4. Merges execute bottom-up\n5. Root merge produces doc\n6. Challenge review\n7. Job completes\n\n**Assertions**:\n- Chunk count matches expected for size\n- Merge DAG has multiple levels\n- Fan-in limits respected\n- All chunks covered in final doc\n- No ticket contains large embedded summaries\n- Tickets reference manifest + artifact URIs only\n\n**Performance**:\n- Track ticket count\n- Verify bounded per-ticket size","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:14:14.480351328-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:06.548227178-07:00","closed_at":"2026-01-15T16:35:06.548227178-07:00","close_reason":"Created large program hierarchical merge test with 8 chunks and 3-level merge tree (4+2+1 merges), verifying proper dependency ordering and level-by-level advancement","labels":["integration-test","testing"],"dependencies":[{"issue_id":"atlas-2ks","depends_on_id":"atlas-b7y","type":"blocks","created_at":"2026-01-15T15:14:14.48213347-07:00","created_by":"andrasfe"}]}
{"id":"atlas-2y9","title":"Implement ArtifactStore adapter interface (ABC)","description":"Create an abstract base class for artifact store adapters per spec design principles.\n\n**Interface Requirements**:\n- write_artifact(uri, content, metadata) -\u003e bool\n- read_artifact(uri) -\u003e bytes/str\n- exists(uri) -\u003e bool\n- delete_artifact(uri) -\u003e bool\n- list_artifacts(prefix) -\u003e List[uri]\n\n**Versioning Support**:\n- get_artifact_version(uri) -\u003e version_hash\n- write_if_not_exists(uri, content) -\u003e bool (for idempotency per section 12.2)\n\n**URI Scheme**:\n- Support for logical URI resolution (e.g., job/{job_id}/chunks/{chunk_id}/result.json)\n- Configurable base path/bucket\n\n**Content Types**:\n- JSON serialization/deserialization helpers\n- YAML serialization/deserialization helpers\n- Raw binary support (for source files)\n\n**Manifest Operations**:\n- read_manifest(uri) -\u003e Manifest\n- write_manifest(uri, manifest) -\u003e bool\n- atomic_update_manifest(uri, updater_fn) -\u003e bool\n\n**Cycle/Iteration Support** (section 11):\n- URI generation with cycle number\n- List artifacts by cycle","status":"closed","priority":0,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:08:53.644066665-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:31:20.355584356-07:00","closed_at":"2026-01-15T15:31:20.355584356-07:00","close_reason":"ArtifactStore adapter ABC fully implemented in src/atlas/adapters/artifact_store.py with all required methods","labels":["adapter","architecture","interface"],"dependencies":[{"issue_id":"atlas-2y9","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:08:53.645881157-07:00","created_by":"andrasfe"}]}
{"id":"atlas-332","title":"Create mock LLM response fixtures","description":"Create deterministic mock LLM responses for testing.\n\n**Chunk Analysis Responses**:\nFor each COBOL fixture, create expected chunk results:\n- Summaries\n- Facts (symbols, calls, IO)\n- Evidence\n- Open questions\n- Confidence scores\n\n**Merge Responses**:\n- Consolidated facts\n- Conflict examples\n- Narrative sections\n\n**Challenge Responses**:\n- No issues scenario\n- Single specific issue\n- Multiple issues with routing hints\n- Cross-cutting issue (no specific scope)\n- Severity mix (BLOCKER, MAJOR, MINOR)\n\n**Follow-up Responses**:\n- Clear answers\n- Additional questions\n- Low confidence answers\n\n**Response Format**:\n- JSON files matching output schemas\n- Keyed by fixture + scenario\n- Deterministic for reproducibility\n\n**Usage**:\nMockLLMAdapter loads responses by key:\n`responses/{fixture_name}/{work_type}/{scenario}.json`","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:15:27.054501021-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:27:06.492560392-07:00","closed_at":"2026-01-15T16:27:06.492560392-07:00","close_reason":"Mock LLM fixtures created","labels":["fixtures","llm","testing"],"dependencies":[{"issue_id":"atlas-332","depends_on_id":"atlas-iy0","type":"blocks","created_at":"2026-01-15T15:15:27.056094538-07:00","created_by":"andrasfe"},{"issue_id":"atlas-332","depends_on_id":"atlas-0ue","type":"blocks","created_at":"2026-01-15T15:15:27.057559766-07:00","created_by":"andrasfe"}]}
{"id":"atlas-3q3","title":"Add get_splitter convenience function to atlas.splitter","description":"The atlas.splitter module exports get_default_registry() but not a direct get_splitter() convenience function. Current usage requires:\n\n```python\nfrom atlas.splitter import get_default_registry\nregistry = get_default_registry()\nsplitter = registry.get_splitter('cobol')\n```\n\nProposed: Add a convenience function:\n```python\nfrom atlas.splitter import get_splitter\nsplitter = get_splitter('cobol')  # Uses default registry internally\n```\n\nThe __init__.py docstring shows this pattern but the function doesn't exist.","status":"open","priority":3,"issue_type":"feature","owner":"andrasf94@gmail.com","created_at":"2026-01-15T19:52:31.742828-07:00","created_by":"andrasfe","updated_at":"2026-01-15T19:52:38.985708-07:00"}
{"id":"atlas-3su","title":"Unit tests: Worker interfaces (Scribe, Aggregator, Challenger)","description":"Write unit tests for all worker interfaces using mock LLM.\n\n**Scribe Worker Tests**:\n- Claim chunk ticket\n- Process chunk with mock LLM\n- Produce valid ChunkResult\n- Handle open_questions generation\n- Idempotency: skip if result exists\n\n**Aggregator Worker Tests**:\n- Claim merge ticket\n- Load input artifacts\n- Merge facts correctly\n- Detect and record conflicts\n- Root merge produces doc + doc_model\n\n**Challenger Worker Tests**:\n- Claim challenge ticket\n- Review doc with mock LLM\n- Produce valid ChallengeResult\n- Generate routing hints\n- Resolution plan is actionable\n\n**Follow-up Worker Tests**:\n- Claim followup ticket\n- Analyze bounded scope\n- Produce valid FollowupAnswer\n- Evidence references are correct\n\n**Patch Merge Tests**:\n- Apply answers to doc\n- Update doc_model traceability\n- Track changes correctly\n\n**Mock LLM Adapter**:\n- Deterministic responses for reproducible tests\n- Configurable outputs per scenario\n\n**Minimum Coverage**: 85% for workers module","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:13:47.375542873-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:27:06.457274962-07:00","closed_at":"2026-01-15T16:27:06.457274962-07:00","close_reason":"Worker unit tests added","labels":["testing","unit-test"],"dependencies":[{"issue_id":"atlas-3su","depends_on_id":"atlas-1yb","type":"blocks","created_at":"2026-01-15T15:13:47.377063975-07:00","created_by":"andrasfe"},{"issue_id":"atlas-3su","depends_on_id":"atlas-apq","type":"blocks","created_at":"2026-01-15T15:13:47.378483487-07:00","created_by":"andrasfe"},{"issue_id":"atlas-3su","depends_on_id":"atlas-222","type":"blocks","created_at":"2026-01-15T15:13:47.379831655-07:00","created_by":"andrasfe"},{"issue_id":"atlas-3su","depends_on_id":"atlas-6b8","type":"blocks","created_at":"2026-01-15T15:13:47.381176656-07:00","created_by":"andrasfe"},{"issue_id":"atlas-3su","depends_on_id":"atlas-z4k","type":"blocks","created_at":"2026-01-15T15:13:47.38252806-07:00","created_by":"andrasfe"},{"issue_id":"atlas-3su","depends_on_id":"atlas-iy0","type":"blocks","created_at":"2026-01-15T15:13:47.383864867-07:00","created_by":"andrasfe"}]}
{"id":"atlas-4ph","title":"Implement Phase F: Re-challenge loop","description":"Implement the optional re-challenge loop per spec section 8 Phase F.\n\n**Re-challenge Conditions**:\nAfter patch merge completes, controller MAY re-run DOC_CHALLENGE until:\n1. Issues resolved (no issues above threshold)\n2. Iteration limit reached\n3. Only minor issues remain (policy-driven)\n\n**Implementation**:\n1. Read review_policy from manifest:\n   - max_iterations\n   - acceptable_severity_levels\n   - min_confidence_threshold\n2. After patch merge, create new DOC_CHALLENGE ticket if:\n   - current_iteration \u003c max_iterations\n   - previous challenge had issues above threshold\n3. New challenge reviews updated doc at cycle N paths\n4. Loop continues until termination condition met\n\n**Cycle Management**:\n- Track cycle_number across iterations\n- Each re-challenge increments cycle\n- Artifacts stored in versioned paths\n\n**Finalization**:\nWhen loop terminates:\n- Create DOC_FINALIZE if configured\n- Mark job DONE with final status\n- Record reason for termination","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:11:14.555744047-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:25.772993199-07:00","closed_at":"2026-01-15T16:35:25.772993199-07:00","close_reason":"Implemented Phase F: Re-challenge loop. The _execute_phase_f method triggers re-challenge after patch merge completes, respecting max_challenge_iterations config limit. Checks previous challenge results to avoid unnecessary re-challenges when no actionable issues remain. Enhanced _determine_phase to properly detect rechallenge state.","labels":["controller","feature","phase-f"],"dependencies":[{"issue_id":"atlas-4ph","depends_on_id":"atlas-a5u","type":"blocks","created_at":"2026-01-15T15:11:14.557578306-07:00","created_by":"andrasfe"}]}
{"id":"atlas-4ps","title":"Implement generic splitter interface for non-COBOL artifacts","description":"Create an abstract splitter interface to support artifact types beyond COBOL.\n\n**Spec Reference**:\nSpec mentions COBOL, COPYBOOK, JCL, OTHER artifact types.\n\n**Splitter Interface**:\n```python\nclass Splitter(ABC):\n    def split(artifact_content: str, profile: SplitterProfile) -\u003e List[ChunkSpec]\n    def get_artifact_types() -\u003e List[str]  # Which types this splitter handles\n```\n\n**Splitter Registry**:\n- Register splitters by artifact_type\n- Fallback to line-based splitting for unknown types\n\n**JCL Splitter** (future):\n- Split by JOB, EXEC, DD statements\n- Semantic boundaries for JCL\n\n**Line-Based Fallback**:\n- Simple line count chunking\n- Respects context_budget\n- Stable chunk boundaries\n\n**Extensibility**:\n- Plugin architecture for custom splitters\n- Configuration per artifact type","status":"closed","priority":3,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:16:04.728928758-07:00","created_by":"andrasfe","updated_at":"2026-01-15T17:28:05.147495-07:00","closed_at":"2026-01-15T17:28:05.147495-07:00","close_reason":"Closed","labels":["chunking","extensibility","feature"],"dependencies":[{"issue_id":"atlas-4ps","depends_on_id":"atlas-d7c","type":"blocks","created_at":"2026-01-15T15:16:04.730655245-07:00","created_by":"andrasfe"}]}
{"id":"atlas-5dn","title":"Implement retry policy and failure handling","description":"Implement retry and failure handling across the system.\n\n**Ticket Retry Policy**:\n- Configurable max_retries per work_type\n- Exponential backoff for transient failures\n- Track retry_count on tickets\n- FAILED -\u003e READY transition for retry\n- Permanent failure after max_retries\n\n**Failure Categories**:\n1. Transient (retry): network errors, timeouts, rate limits\n2. Permanent (no retry): invalid input, schema errors\n3. Partial (special handling): chunk failed but others can continue\n\n**Merge Failure Handling**:\n- If input chunk FAILED, merge can proceed with partial inputs\n- Record missing inputs in coverage\n- Challenger may flag gaps\n\n**Challenge Loop Failure**:\n- If challenger fails, can re-run on same doc\n- If follow-ups fail, track which issues unaddressed\n\n**Circuit Breaker**:\n- If failure rate exceeds threshold, pause job\n- Alert for manual intervention\n- Resume capability after fix\n\n**Metrics**:\n- Track failure rates by work_type\n- Alert on anomalous failure patterns","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:12:46.167221888-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:27:06.423232854-07:00","closed_at":"2026-01-15T16:27:06.423232854-07:00","close_reason":"Retry policy implemented","labels":["feature","infrastructure","reliability"],"dependencies":[{"issue_id":"atlas-5dn","depends_on_id":"atlas-12g","type":"blocks","created_at":"2026-01-15T15:12:46.169046198-07:00","created_by":"andrasfe"},{"issue_id":"atlas-5dn","depends_on_id":"atlas-vma","type":"blocks","created_at":"2026-01-15T15:12:46.17041262-07:00","created_by":"andrasfe"}]}
{"id":"atlas-6b8","title":"Implement Follow-up worker for targeted issue investigation","description":"Create the Follow-up worker for DOC_FOLLOWUP tickets per spec section 6.6.\n\n**Follow-up Responsibilities**:\n- Claim DOC_FOLLOWUP tickets\n- Analyze bounded scope to answer specific challenger question\n- Produce Follow-up Answer artifact\n- Write to output_uri\n- Mark ticket DONE\n\n**Worker Interface**:\n```python\nclass FollowupWorker:\n    def claim_followup(self) -\u003e Optional[WorkItem]\n    def investigate_issue(self, work_item: WorkItem) -\u003e FollowupAnswer\n    def complete_followup(self, work_id, result: FollowupAnswer)\n```\n\n**Scope Handling**:\nPayload scope can be:\n- chunk_id(s)\n- paragraph list\n- line ranges\n- cross-cutting query plan\n\nWorker must respect scope bounds and not exceed context budget.\n\n**Output Requirements** (section 7.5):\nProduce FollowupAnswer with:\n- issue_id\n- scope (what was analyzed)\n- answer (clear text)\n- facts (structured, mergeable)\n- evidence (line ranges/refs)\n- confidence\n\n**Context Assembly**:\n- Load relevant chunk results from inputs URIs\n- Load source slices if needed\n- Stay within context budget","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:11:56.601872135-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:49:14.668722482-07:00","closed_at":"2026-01-15T15:49:14.668722482-07:00","close_reason":"Implemented FollowupWorker for targeted issue investigation with bounded scope analysis, evidence extraction, fact gathering, and cross-cutting support. All worker tests pass.","labels":["feature","followup","worker"],"dependencies":[{"issue_id":"atlas-6b8","depends_on_id":"atlas-fu3","type":"blocks","created_at":"2026-01-15T15:11:56.603611917-07:00","created_by":"andrasfe"},{"issue_id":"atlas-6b8","depends_on_id":"atlas-2y9","type":"blocks","created_at":"2026-01-15T15:11:56.604960887-07:00","created_by":"andrasfe"},{"issue_id":"atlas-6b8","depends_on_id":"atlas-p00","type":"blocks","created_at":"2026-01-15T15:11:56.606270122-07:00","created_by":"andrasfe"}]}
{"id":"atlas-6mc","title":"Integration test: Challenger loop with cross-cutting question","description":"Create integration test for challenger loop per acceptance test scenario 3.\n\n**Scenario** (spec section 15):\nChallenger asks cross-cutting question ('Explain error handling for file reads/writes').\nController routes into bounded follow-ups across relevant PROCEDURE chunks.\nPatch merge updates the error-handling section and doc model traceability.\n\n**Test Setup**:\n- COBOL program with error handling across multiple paragraphs\n- Mock challenger that produces cross-cutting issue\n- Mock scribe for follow-ups\n\n**Test Flow**:\n1. Complete initial documentation\n2. Challenger raises cross-cutting issue about error handling\n3. Controller uses routing algorithm\n4. Multiple bounded follow-ups created (not one giant task)\n5. Follow-ups answered\n6. Patch merge updates documentation\n7. Re-challenge shows issue resolved\n\n**Assertions**:\n- Follow-ups target specific chunks/divisions\n- No follow-up exceeds scope limits\n- Patch merge correctly updates affected sections\n- Doc model traceability updated\n- Issue marked resolved after re-challenge","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:14:36.752432347-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:22.862946075-07:00","closed_at":"2026-01-15T16:35:22.862946075-07:00","close_reason":"Created challenger loop with cross-cutting question test verifying empty suspected_scopes routing through split_cross_cutting_scope","labels":["challenger","integration-test","testing"],"dependencies":[{"issue_id":"atlas-6mc","depends_on_id":"atlas-bd7","type":"blocks","created_at":"2026-01-15T15:14:36.75426358-07:00","created_by":"andrasfe"},{"issue_id":"atlas-6mc","depends_on_id":"atlas-b7y","type":"blocks","created_at":"2026-01-15T15:14:36.755954521-07:00","created_by":"andrasfe"}]}
{"id":"atlas-799","title":"Integration test: Crash recovery and resume","description":"Create integration test for crash recovery per acceptance test scenario 2.\n\n**Scenario** (spec section 15):\nKill workers mid-run; restart; no duplicate chunk tickets; merge resumes.\n\n**Test Setup**:\n- Medium COBOL program\n- Simulated crash points\n- State persistence verification\n\n**Test Flow**:\n1. Start job, process some chunks\n2. Simulate crash (clear in-memory state but keep artifacts)\n3. Reload from persisted state\n4. Resume reconcile loop\n5. Verify no duplicate tickets\n6. Continue to completion\n\n**Crash Points to Test**:\n- Mid chunk processing\n- After some chunks, before merges\n- During merge phase\n- After challenge, before followups\n\n**Assertions**:\n- Idempotency keys prevent duplicates\n- Already-DONE work not repeated\n- Orphan IN_PROGRESS tickets handled\n- Job completes correctly\n- Final result same as uninterrupted run","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:14:26.654224531-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:14.351298604-07:00","closed_at":"2026-01-15T16:35:14.351298604-07:00","close_reason":"Created crash recovery and resume test using JobStatePersistence for checkpoint creation/restore, verifying workflow can continue after simulated crash","labels":["integration-test","reliability","testing"],"dependencies":[{"issue_id":"atlas-799","depends_on_id":"atlas-0ug","type":"blocks","created_at":"2026-01-15T15:14:26.667922875-07:00","created_by":"andrasfe"},{"issue_id":"atlas-799","depends_on_id":"atlas-b7y","type":"blocks","created_at":"2026-01-15T15:14:26.673069815-07:00","created_by":"andrasfe"}]}
{"id":"atlas-86u","title":"Implement CI/CD test pipeline configuration","description":"Configure CI/CD pipeline for automated testing.\n\n**Test Stages**:\n1. Lint (flake8, mypy type checking)\n2. Unit tests with coverage report\n3. Integration tests\n4. Coverage threshold enforcement\n\n**Coverage Requirements**:\n- Core models: 95%\n- Adapters: 95%\n- Controller: 90%\n- Workers: 85%\n- Overall: 90%\n\n**Test Configuration**:\n- pytest configuration\n- pytest-cov for coverage\n- pytest-asyncio if async\n- Test markers (unit, integration, slow)\n\n**Artifacts**:\n- Coverage reports (HTML, XML)\n- Test results (JUnit XML)\n- Failure screenshots/logs\n\n**Performance Tests** (optional):\n- Run large program tests with timing\n- Track regression in processing time","status":"closed","priority":3,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:15:34.670977816-07:00","created_by":"andrasfe","updated_at":"2026-01-15T17:24:48.594815-07:00","closed_at":"2026-01-15T17:24:48.594815-07:00","close_reason":"Closed","labels":["ci-cd","infrastructure","testing"],"dependencies":[{"issue_id":"atlas-86u","depends_on_id":"atlas-jwa","type":"blocks","created_at":"2026-01-15T15:15:34.672879672-07:00","created_by":"andrasfe"},{"issue_id":"atlas-86u","depends_on_id":"atlas-b7y","type":"blocks","created_at":"2026-01-15T15:15:34.674390945-07:00","created_by":"andrasfe"}]}
{"id":"atlas-9wd","title":"Unit tests: Merge DAG planner","description":"Write comprehensive unit tests for the merge DAG planner.\n\n**Test Cases**:\n- Small program: single merge level\n- Medium program: two merge levels\n- Large program: three+ merge levels\n- Uneven chunk distribution\n- Different chunk kinds\n\n**DAG Validation Tests**:\n- No cycles (topological sort succeeds)\n- Bounded fan-in (8-20 inputs)\n- All chunks covered by some merge\n- Root node exists and is unique\n- Dependency edges are correct\n\n**Boundary Conditions**:\n- Exactly fan-in limit inputs\n- Just over fan-in limit (forces new level)\n- Single chunk program\n- Maximum merge depth\n\n**Determinism Tests**:\n- Same chunks = identical DAG\n- merge_node_ids are stable\n\n**Output Structure**:\n- Correct levels assigned\n- merge_node_ids are unique\n- Input references are valid chunk_ids or merge_node_ids\n\n**Minimum Coverage**: 90% for planner module","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:13:15.341840832-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:12:53.889472797-07:00","closed_at":"2026-01-15T16:12:53.889472797-07:00","close_reason":"Comprehensive unit tests for DAG planner. 100+ tests covering merge DAG construction, validation, semantic grouping.","labels":["testing","unit-test"],"dependencies":[{"issue_id":"atlas-9wd","depends_on_id":"atlas-ekf","type":"blocks","created_at":"2026-01-15T15:13:15.343320256-07:00","created_by":"andrasfe"}]}
{"id":"atlas-a5u","title":"Implement Phase E: Follow-up dispatch and patch merge","description":"Implement Phase E per spec section 8 Phase E.\n\n**Follow-up Dispatch**:\n1. Controller uses routing algorithm to convert issues to DOC_FOLLOWUP tickets\n2. Each follow-up has bounded scope from routing\n3. Create follow-up tickets (READY)\n4. Scribes answer follow-ups, produce follow-up answer artifacts\n\n**Patch Merge**:\nWhen required follow-ups complete:\n1. Controller creates DOC_PATCH_MERGE ticket\n2. Patch merge payload includes:\n   - base_doc_uri + base_doc_model_uri\n   - follow-up answer URIs as inputs\n   - output_doc_uri + output_doc_model_uri\n3. Aggregator applies answers to documentation\n4. Update doc and doc_model artifacts\n\n**Iteration Tracking** (section 11):\n- Increment cycle_number\n- Store artifacts in cycle-specific paths\n- Track which issues were addressed\n\n**Event Logging** (section 13):\n- Log follow-up dispatch count per issue\n- Track sections changed in patch merge\n- Record issues addressed","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:11:05.408051306-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:20.075248844-07:00","closed_at":"2026-01-15T16:35:20.075248844-07:00","close_reason":"Implemented Phase E: Follow-up dispatch and patch merge. The _execute_phase_e method creates follow-ups for BLOCKER/MAJOR issues from challenge results. The _execute_patch_merge method creates patch merge work items when all follow-ups complete. Proper cycle number tracking ensures correct workflow progression.","labels":["controller","feature","phase-e"],"dependencies":[{"issue_id":"atlas-a5u","depends_on_id":"atlas-iqm","type":"blocks","created_at":"2026-01-15T15:11:05.409746945-07:00","created_by":"andrasfe"}]}
{"id":"atlas-apq","title":"Implement Aggregator worker interface and merge handler","description":"Create the Aggregator (merger) worker interface per spec section 3.3.\n\n**Aggregator Responsibilities**:\n- Claim DOC_MERGE tickets\n- Read input artifacts (chunk results or child merges)\n- Merge into higher-level summary\n- Produce Merge Result artifact\n- Write to output_uri\n- Mark ticket DONE\n\n**Worker Interface**:\n```python\nclass AggregatorWorker:\n    def claim_next_merge(self) -\u003e Optional[WorkItem]\n    def process_merge(self, work_item: WorkItem) -\u003e MergeResult\n    def complete_merge(self, work_id, result: MergeResult)\n```\n\n**Merge Requirements** (section 7.2):\nProduce MergeResult with:\n- coverage (included/missing inputs)\n- consolidated_facts (merged call graph, IO map, error handling)\n- conflicts (disagreements with follow-up scope suggestions)\n- narrative_sections\n\n**Root Merge Special Output**:\nWhen merge_node is root:\n- Produce doc_uri (rendered documentation)\n- Produce doc_model_uri (traceability model per section 7.3)\n\n**Merge Strategies**:\n- Facts union with conflict detection\n- Call graph edge consolidation\n- IO operation deduplication\n- Error handling pattern aggregation\n\n**Context Budget Compliance**:\nEnsure merged inputs fit context budget before LLM call","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:11:35.859070913-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:49:12.411671832-07:00","closed_at":"2026-01-15T15:49:12.411671832-07:00","close_reason":"Implemented AggregatorWorker with fact deduplication, conflict detection, narrative generation via LLM, and patch application for follow-up answers. All worker tests pass.","labels":["aggregator","feature","worker"],"dependencies":[{"issue_id":"atlas-apq","depends_on_id":"atlas-fu3","type":"blocks","created_at":"2026-01-15T15:11:35.86081882-07:00","created_by":"andrasfe"},{"issue_id":"atlas-apq","depends_on_id":"atlas-2y9","type":"blocks","created_at":"2026-01-15T15:11:35.862166938-07:00","created_by":"andrasfe"},{"issue_id":"atlas-apq","depends_on_id":"atlas-p00","type":"blocks","created_at":"2026-01-15T15:11:35.863427411-07:00","created_by":"andrasfe"}]}
{"id":"atlas-au2","title":"Define canonical status model enum and transitions","description":"Implement the canonical status model from spec section 5:\n\n**Statuses**:\n- NEW - created, not eligible to claim\n- READY - eligible to claim\n- IN_PROGRESS - claimed/leased by a worker\n- BLOCKED - waiting for dependencies/inputs\n- DONE - completed successfully\n- FAILED - completed with error; may be retried\n- CANCELED - no longer needed (optional)\n\n**Requirements**:\n- Create Status enum with all values\n- Implement valid transition rules:\n  - Workers MUST NOT start on BLOCKED items\n  - Merge/patch merge items MUST be BLOCKED until inputs are DONE\n- Create transition validation helper\n- Document mapping guidance for external ticket systems","status":"closed","priority":0,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:08:02.231486456-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:28:23.300035014-07:00","closed_at":"2026-01-15T15:28:23.300035014-07:00","close_reason":"Implemented canonical status model enum with valid transitions, WorkItemType, ArtifactType, ChunkKind, and IssueSeverity enums","labels":["architecture","core","models"]}
{"id":"atlas-azd","title":"Implement Configuration management","description":"Create configuration system for the orchestration framework.\n\n**Configurable Parameters**:\n\n**Context/Budget**:\n- context_budget (default token limit)\n- overhead_budget (prompt/context ref tokens)\n\n**Chunking**:\n- min_chunk_lines\n- prefer_semantic_boundaries\n\n**Merge DAG**:\n- max_fan_in (8-20)\n- min_fan_in\n\n**Retry Policy**:\n- max_retries per work_type\n- backoff_multiplier\n- initial_delay\n\n**Challenger**:\n- max_iterations\n- acceptable_severity_levels\n- challenge_profile defaults\n\n**Lease/Claim**:\n- lease_timeout\n- heartbeat_interval\n\n**Configuration Sources**:\n- YAML/JSON config file\n- Environment variables\n- Programmatic overrides\n\n**Validation**:\n- Schema validation on load\n- Reasonable defaults\n- Error messages for invalid config","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:15:44.14247841-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:27:06.526276469-07:00","closed_at":"2026-01-15T16:27:06.526276469-07:00","close_reason":"Config management implemented","labels":["configuration","feature","infrastructure"],"dependencies":[{"issue_id":"atlas-azd","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:15:44.14421727-07:00","created_by":"andrasfe"}]}
{"id":"atlas-b7y","title":"Integration test: End-to-end small COBOL program","description":"Create integration test for a small COBOL program that fits acceptance test scenario 1.\n\n**Scenario** (spec section 15):\nSmall COBOL file processed end-to-end.\n\n**Test Setup**:\n- Sample COBOL program (~200 lines)\n- In-memory ticket system\n- Filesystem artifact store\n- Mock LLM adapter with deterministic outputs\n\n**Test Flow**:\n1. Create DOC_REQUEST for sample program\n2. Run controller reconcile loop\n3. Verify chunk tickets created\n4. Simulate scribe processing all chunks\n5. Run reconcile - verify merges unblock\n6. Simulate aggregator processing merges\n7. Run reconcile - verify challenge created\n8. Simulate challenger with no issues\n9. Verify job completes successfully\n\n**Assertions**:\n- All phases execute in order\n- Correct number of chunks/merges\n- Final doc artifact exists\n- Doc model has correct structure\n- No duplicate tickets created\n\n**Determinism**:\n- Run twice, get identical results","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:14:05.572245427-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:05.894350865-07:00","closed_at":"2026-01-15T16:35:05.894350865-07:00","close_reason":"Created end-to-end small COBOL program test verifying full workflow from DOC_REQUEST through chunk analysis, merge, challenge, and completion phases","labels":["integration-test","testing"],"dependencies":[{"issue_id":"atlas-b7y","depends_on_id":"atlas-ctd","type":"blocks","created_at":"2026-01-15T15:14:05.57385749-07:00","created_by":"andrasfe"},{"issue_id":"atlas-b7y","depends_on_id":"atlas-3su","type":"blocks","created_at":"2026-01-15T15:14:05.575322537-07:00","created_by":"andrasfe"},{"issue_id":"atlas-b7y","depends_on_id":"atlas-c6s","type":"blocks","created_at":"2026-01-15T15:14:05.576754772-07:00","created_by":"andrasfe"}]}
{"id":"atlas-bd7","title":"Unit tests: Issue routing algorithm","description":"Write comprehensive unit tests for the challenger issue routing algorithm.\n\n**Routing Strategy Tests**:\n1. suspected_scopes with chunk_ids -\u003e direct routing\n2. Doc section source_refs -\u003e chunk extraction\n3. Symbol references -\u003e index lookup\n4. Cross-cutting issues -\u003e bounded plan generation\n\n**Scope Constraint Tests**:\n- Single chunk scope\n- Max N chunks (configurable limit)\n- Merge node scope\n- Division-bounded scope\n\n**Cross-Cutting Plan Tests**:\n- Error handling across PROCEDURE -\u003e multiple bounded followups\n- Issue-specific merge created\n- No single giant task\n\n**Index Lookup Tests**:\n- Symbol -\u003e chunk mapping\n- Paragraph -\u003e chunk mapping\n- File -\u003e chunk mapping\n- Missing symbol handling\n\n**Edge Cases**:\n- No routing hints (fallback strategy)\n- Conflicting hints\n- Unknown symbols\n- Empty scopes\n\n**Output Validation**:\n- Follow-up payloads are valid\n- Scopes are within bounds\n- All issues have routing plan\n\n**Minimum Coverage**: 90% for routing module","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:13:36.605135909-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:31.988899485-07:00","closed_at":"2026-01-15T16:35:31.988899485-07:00","close_reason":"Created comprehensive unit tests for issue routing algorithm. Tests in tests/unit/controller/test_issue_routing.py cover all 4 routing priorities, scope size constraints, cross-cutting split logic, and integration tests. Also added tests/unit/controller/test_phases.py with tests for Phases C, D, E, F and phase determination logic. All 90 controller tests pass.","labels":["testing","unit-test"],"dependencies":[{"issue_id":"atlas-bd7","depends_on_id":"atlas-iqm","type":"blocks","created_at":"2026-01-15T15:13:36.606922258-07:00","created_by":"andrasfe"}]}
{"id":"atlas-c19","title":"Implement Phase B: Chunk analysis completion tracking","description":"Implement Phase B tracking per spec section 8 Phase B.\n\n**Phase B Flow**:\n1. Scribes claim DOC_CHUNK tickets (READY)\n2. Scribe produces chunk result artifact\n3. Scribe marks ticket DONE\n\n**Controller Responsibilities**:\n- Monitor chunk ticket completions\n- Track progress (% chunks complete)\n- Handle FAILED chunks (retry policy)\n- When chunk completes, check if any merge tickets can be unblocked\n\n**Merge Unblocking Logic**:\nFor each merge node in manifest:\n1. Get input IDs (chunk_ids or child merge_node_ids)\n2. Query ticket statuses for all inputs\n3. If ALL inputs are DONE -\u003e transition merge to READY\n4. If ANY input is FAILED -\u003e handle per retry policy\n\n**Events/Observability** (section 13):\n- Log chunk completion with timing\n- Track cumulative progress\n- Alert on stuck/failed chunks","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:10:25.570832013-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:27:06.390037904-07:00","closed_at":"2026-01-15T16:27:06.390037904-07:00","close_reason":"Phase B handler implemented","labels":["controller","feature","phase-b"],"dependencies":[{"issue_id":"atlas-c19","depends_on_id":"atlas-12g","type":"blocks","created_at":"2026-01-15T15:10:25.577197778-07:00","created_by":"andrasfe"},{"issue_id":"atlas-c19","depends_on_id":"atlas-0va","type":"blocks","created_at":"2026-01-15T15:10:25.581257099-07:00","created_by":"andrasfe"}]}
{"id":"atlas-c6s","title":"Unit tests: Adapters (TicketSystem, ArtifactStore)","description":"Write unit tests for adapter implementations.\n\n**In-Memory TicketSystem Tests**:\n- Create, read, update, delete tickets\n- Status transitions (valid and invalid)\n- Idempotency key enforcement\n- Claim/release with lease semantics\n- Dependency queries\n- Concurrent access (thread safety)\n- Reset and seed utilities\n\n**Filesystem ArtifactStore Tests**:\n- Write and read artifacts\n- URI resolution\n- Atomic writes\n- JSON/YAML serialization\n- Exists checks\n- List artifacts by prefix\n- Cycle-aware paths\n- write_if_not_exists behavior\n- Directory creation\n- Error handling (permissions, disk full simulation)\n\n**Interface Compliance Tests**:\n- Both adapters satisfy ABC contracts\n- All methods implemented\n- Return types correct\n\n**Minimum Coverage**: 95% for adapters module","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:13:56.370689323-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:12:53.959491761-07:00","closed_at":"2026-01-15T16:12:53.959491761-07:00","close_reason":"Unit tests for adapters including MemoryTicketSystem, FilesystemStore. High coverage achieved.","labels":["testing","unit-test"],"dependencies":[{"issue_id":"atlas-c6s","depends_on_id":"atlas-rt7","type":"blocks","created_at":"2026-01-15T15:13:56.37232496-07:00","created_by":"andrasfe"},{"issue_id":"atlas-c6s","depends_on_id":"atlas-0n1","type":"blocks","created_at":"2026-01-15T15:13:56.373737759-07:00","created_by":"andrasfe"}]}
{"id":"atlas-ctd","title":"Unit tests: Controller reconcile loop","description":"Write comprehensive unit tests for the controller reconcile loop.\n\n**Uses**: In-memory ticket system adapter, filesystem artifact store\n\n**Phase Transition Tests**:\n- DOC_REQUEST -\u003e creates chunks and merges\n- Chunks complete -\u003e merges unblock\n- Merges complete -\u003e challenge created\n- Challenge complete -\u003e followups or finalize\n\n**Idempotency Tests**:\n- Reconcile twice = no duplicate tickets\n- Partial completion + reconcile = resume correctly\n- Crash simulation + reconcile = correct state\n\n**Dependency Tracking Tests**:\n- Merge stays BLOCKED until all inputs DONE\n- Correct transition to READY\n- Multiple merges unblock in correct order\n\n**Edge Cases**:\n- Single chunk program (no merge needed?)\n- All chunks fail\n- Partial chunk failure\n- Challenge with no issues\n- Challenge with many issues\n\n**State Consistency**:\n- Ticket states match manifest expectations\n- No orphan tickets\n- No missing tickets\n\n**Minimum Coverage**: 90% for controller module","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:13:25.173637402-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:12:53.924488892-07:00","closed_at":"2026-01-15T16:12:53.924488892-07:00","close_reason":"Unit tests for controller reconcile loop. Tests cover state transitions, phase handling, issue routing.","labels":["testing","unit-test"],"dependencies":[{"issue_id":"atlas-ctd","depends_on_id":"atlas-12g","type":"blocks","created_at":"2026-01-15T15:13:25.175247962-07:00","created_by":"andrasfe"},{"issue_id":"atlas-ctd","depends_on_id":"atlas-rt7","type":"blocks","created_at":"2026-01-15T15:13:25.17675596-07:00","created_by":"andrasfe"},{"issue_id":"atlas-ctd","depends_on_id":"atlas-0n1","type":"blocks","created_at":"2026-01-15T15:13:25.178097996-07:00","created_by":"andrasfe"}]}
{"id":"atlas-d7c","title":"Implement COBOL-aware chunk splitter","description":"Create a COBOL-aware splitter that produces deterministic, semantic chunks per spec section 10.1.\n\n**Requirements**:\n- Prefer semantic boundaries (COBOL divisions/sections/paragraphs)\n- Ensure chunks fit within context_budget after overhead\n- Create chunk_kind labels for targeted follow-ups\n\n**COBOL Structure Awareness**:\n- IDENTIFICATION DIVISION\n- ENVIRONMENT DIVISION  \n- DATA DIVISION (WORKING-STORAGE, FILE, LINKAGE sections)\n- PROCEDURE DIVISION (paragraphs/sections)\n\n**Chunk Plan Output**:\nFor each chunk:\n- chunk_id (deterministic, stable)\n- chunk_kind (IDENTIFICATION | ENVIRONMENT | DATA_WORKING | DATA_FILE | DATA_LINKAGE | PROCEDURE_PART_N)\n- chunk_locator:\n  - line_range (start, end)\n  - semantic_locator (division, section, paragraphs)\n\n**Determinism Requirements** (section 2):\n- Same source + splitter_profile = identical chunks\n- Stable chunk_ids across runs\n\n**Splitter Profile**:\n- context_budget (max tokens per chunk)\n- overhead_budget (tokens for prompts/context refs)\n- prefer_semantic (bool)\n- min_chunk_lines (avoid tiny chunks)","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:09:33.65905434-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:36:29.587005467-07:00","closed_at":"2026-01-15T15:36:29.587005467-07:00","close_reason":"COBOL-aware chunk splitter implemented in src/atlas/splitter/cobol.py with COBOLSplitter class that parses DIVISION/SECTION/PARAGRAPH boundaries. Features: deterministic chunking, semantic boundary detection, token budget enforcement, stable chunk ID generation. 25 comprehensive unit tests in tests/unit/test_cobol_splitter.py - all passing","labels":["chunking","cobol","feature"],"dependencies":[{"issue_id":"atlas-d7c","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:09:33.661000188-07:00","created_by":"andrasfe"}]}
{"id":"atlas-dxu","title":"Define work type enums and payload schemas","description":"Implement all work types from spec section 6 with their payload schemas:\n\n**Work Types**:\n1. DOC_REQUEST (6.1) - Root request\n   - Payload: job_id, artifact_ref, analysis_profile, context_budget, splitter_profile, manifest_uri (optional)\n   \n2. DOC_PLAN (6.2) - Optional planning phase\n   - Payload: job_id, artifact_ref, analysis_profile, splitter_profile, context_budget\n   - Output: manifest_uri\n   \n3. DOC_CHUNK (6.3) - Chunk analysis\n   - Payload: job_id, artifact_ref, manifest_uri, chunk_id, chunk_locator, result_uri\n   \n4. DOC_MERGE (6.4) - Merge chunk results\n   - Payload: job_id, artifact_ref, manifest_uri, merge_node_id, input_uris, output_uri\n   \n5. DOC_CHALLENGE (6.5) - Review documentation\n   - Payload: job_id, artifact_ref, doc_uri, doc_model_uri, challenge_profile, output_uri\n   \n6. DOC_FOLLOWUP (6.6) - Issue-driven targeted work\n   - Payload: job_id, artifact_ref, issue_id, scope, inputs, output_uri\n   \n7. DOC_PATCH_MERGE (6.7) - Apply follow-up answers\n   - Payload: job_id, artifact_ref, base_doc_uri, base_doc_model_uri, inputs, output_doc_uri, output_doc_model_uri\n   \n8. DOC_FINALIZE (6.8) - Optional final deliverables\n   - Payload: job_id, artifact_ref, final outputs\n\nCreate Pydantic models with validation for each payload type.","status":"closed","priority":0,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:08:12.705815-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:31:17.412823374-07:00","closed_at":"2026-01-15T15:31:17.412823374-07:00","close_reason":"Work type enums (WorkItemType, WorkItemStatus) and all payload schemas fully implemented in src/atlas/models/enums.py and src/atlas/models/work_item.py with tests passing","labels":["architecture","core","models"],"dependencies":[{"issue_id":"atlas-dxu","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:08:12.707642256-07:00","created_by":"andrasfe"}]}
{"id":"atlas-ekf","title":"Implement merge DAG planner","description":"Create the merge DAG planner that produces a hierarchical merge plan per spec section 10.2.\n\n**Requirements**:\n- Bounded fan-in (8-20 inputs per merge node)\n- Produce a DAG (no cycles)\n- Root merge node produces final doc + doc_model\n- Include merge nodes for large subdivisions (PROCEDURE_PART1..N)\n\n**Algorithm**:\n1. Group chunks by kind/division\n2. Create first-level merges for each group (bounded fan-in)\n3. If needed, create intermediate merges\n4. Create root merge that combines all division-level outputs\n\n**Output (merge_dag in manifest)**:\nFor each merge_node:\n- merge_node_id (deterministic)\n- inputs (chunk_ids or child merge_node_ids)\n- level (0 = leaf merges, higher = closer to root)\n- output_kind (DIVISION_MERGE | INTERMEDIATE | ROOT)\n\n**Dependency Edges**:\n- Each merge depends on its inputs being DONE\n- Controller uses this to set BLOCKED/READY transitions\n\n**Example DAG for Large COBOL**:\n```\nchunks[0-7] -\u003e merge_procedure_part1\nchunks[8-15] -\u003e merge_procedure_part2\nmerge_procedure_part1 + merge_procedure_part2 -\u003e merge_procedure\nmerge_data_division + merge_procedure -\u003e root_merge\n```","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:09:46.134295531-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:46:15.46852328-07:00","closed_at":"2026-01-15T15:46:15.46852328-07:00","close_reason":"Implemented DAGPlanner for hierarchical merge tree construction with configurable fan-in (default 4-8), semantic grouping by COBOL division, balanced tree construction, DAG validation (cycles, root node, fan-in), and comprehensive unit tests (26 tests passing)","labels":["feature","merge","planning"],"dependencies":[{"issue_id":"atlas-ekf","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:09:46.136292154-07:00","created_by":"andrasfe"},{"issue_id":"atlas-ekf","depends_on_id":"atlas-d7c","type":"blocks","created_at":"2026-01-15T15:09:46.137685386-07:00","created_by":"andrasfe"}]}
{"id":"atlas-fu3","title":"Implement TicketSystem adapter interface (ABC)","description":"Create an abstract base class for ticket system adapters per spec sections 2 and 4.2.\n\n**Interface Requirements**:\n- create_ticket(work_item) -\u003e work_id\n- get_ticket(work_id) -\u003e WorkItem\n- update_ticket(work_id, updates) -\u003e WorkItem\n- claim_ticket(work_id, worker_id) -\u003e bool (with lease semantics)\n- release_ticket(work_id) -\u003e bool\n- list_tickets(filters) -\u003e List[WorkItem]\n- transition_status(work_id, new_status) -\u003e bool\n\n**Status Mapping**:\n- Abstract method to map canonical statuses to backend statuses\n- Abstract method to map backend statuses to canonical\n\n**Idempotency Support** (section 12.1):\n- find_by_idempotency_key(key) -\u003e Optional[work_id]\n- create_with_idempotency_key(work_item, key) -\u003e work_id\n\n**Lease/Claim Semantics** (section 12.3):\n- Support for lease expiration\n- Optimistic locking to prevent duplicate work\n\n**Dependency Management**:\n- set_dependencies(work_id, depends_on: List[work_id])\n- get_dependencies(work_id) -\u003e List[work_id]\n- check_dependencies_complete(work_id) -\u003e bool","status":"closed","priority":0,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:08:44.494989888-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:31:18.841620788-07:00","closed_at":"2026-01-15T15:31:18.841620788-07:00","close_reason":"TicketSystem adapter ABC fully implemented in src/atlas/adapters/ticket_system.py with all required methods","labels":["adapter","architecture","interface"],"dependencies":[{"issue_id":"atlas-fu3","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:08:44.49672915-07:00","created_by":"andrasfe"},{"issue_id":"atlas-fu3","depends_on_id":"atlas-au2","type":"blocks","created_at":"2026-01-15T15:08:44.497964706-07:00","created_by":"andrasfe"}]}
{"id":"atlas-ijb","title":"Integration test: Mock agent simulation harness","description":"Create a test harness that simulates multiple agents working concurrently.\n\n**Purpose**:\nTest the full system with simulated agent behavior to verify:\n- Concurrent ticket claiming\n- Race condition handling\n- Lease expiration\n- Progress tracking\n\n**Harness Components**:\n- MockScribeAgent: claims and processes chunks\n- MockAggregatorAgent: claims and processes merges\n- MockChallengerAgent: reviews documentation\n- WorkerPool: manages concurrent execution\n\n**Configuration**:\n- Number of concurrent workers per type\n- Processing delay simulation\n- Failure injection probability\n- Lease timeout behavior\n\n**Test Scenarios**:\n1. Multiple scribes competing for chunks\n2. Aggregator waiting for merge inputs\n3. Lease expiration and re-claim\n4. Worker failure mid-processing\n\n**Assertions**:\n- No duplicate processing\n- All work eventually completes\n- Correct final state\n- Metrics accurate","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:15:06.48935095-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:21.728733471-07:00","closed_at":"2026-01-15T16:35:21.728733471-07:00","close_reason":"Created comprehensive mock agent simulation harness in tests/integration/mock_agent_harness.py with configurable delays, failures, and response generation for all agent types","labels":["infrastructure","integration-test","testing"],"dependencies":[{"issue_id":"atlas-ijb","depends_on_id":"atlas-b7y","type":"blocks","created_at":"2026-01-15T15:15:06.491095561-07:00","created_by":"andrasfe"},{"issue_id":"atlas-ijb","depends_on_id":"atlas-3su","type":"blocks","created_at":"2026-01-15T15:15:06.492774389-07:00","created_by":"andrasfe"}]}
{"id":"atlas-iqm","title":"Implement challenger issue routing algorithm","description":"Implement the issue-to-chunk routing algorithm per spec section 9.\n\n**Section 9.1 - Inputs for Routing**:\n1. doc_model.sections[].source_refs (best)\n2. issue.suspected_scopes (from challenger)\n3. routing_hints (symbols/paragraphs/files)\n4. Manifest chunk boundaries\n5. Chunk result indexes\n\n**Section 9.2 - Routing Algorithm**:\nFor each issue:\n1. If suspected_scopes contains chunk_ids -\u003e create follow-ups per chunk\n2. Else if issue references doc sections with source_refs -\u003e use those chunk_ids\n3. Else if issue references symbols/paragraphs -\u003e consult indexes\n4. Else: create bounded cross-cutting follow-up plan\n\n**Cross-Cutting Plan** (section 9.3):\n- One follow-up per merge-level or per division\n- Plus issue-specific merge to consolidate answers\n\n**Scope Size Constraints**:\nFollow-up MUST target one of:\n- 1 chunk\n- Small list of chunks (max configurable, 3-5)\n- A merge node output\n- Single concern within one division\n\n**Never** create one giant task for 'whole program re-analysis'\n\n**Output**:\n- List of DOC_FOLLOWUP ticket payloads with bounded scopes","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:10:55.811087061-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:13.614517135-07:00","closed_at":"2026-01-15T16:35:13.614517135-07:00","close_reason":"Implemented challenger issue routing algorithm per spec section 9.2. Routes issues via 4 priorities: 1) suspected_scopes with chunk IDs, 2) doc section source refs, 3) routing hints (symbols/paragraphs/files) via indexes, 4) cross-cutting bounded follow-up plan. Respects max 5 chunks per scope per spec 9.3.","labels":["controller","feature","routing"],"dependencies":[{"issue_id":"atlas-iqm","depends_on_id":"atlas-m98","type":"blocks","created_at":"2026-01-15T15:10:55.812903236-07:00","created_by":"andrasfe"},{"issue_id":"atlas-iqm","depends_on_id":"atlas-p00","type":"blocks","created_at":"2026-01-15T15:10:55.814265119-07:00","created_by":"andrasfe"}]}
{"id":"atlas-iy0","title":"Implement LLM adapter interface (ABC)","description":"Create an abstract interface for LLM integrations.\n\n**Interface Requirements**:\nWorkers need LLM capabilities but spec is LLM-agnostic.\n\n```python\nclass LLMAdapter(ABC):\n    def complete(self, prompt: str, context: str, output_schema: Type) -\u003e Any\n    def estimate_tokens(self, text: str) -\u003e int\n    def get_context_limit(self) -\u003e int\n```\n\n**Context Budget Enforcement**:\n- Check token count before LLM call\n- Truncate or fail gracefully if over budget\n- Report actual token usage\n\n**Structured Output**:\n- Support typed output schemas\n- Parse LLM response into structured data\n- Handle parsing failures\n\n**Configuration**:\n- Model selection\n- Temperature/sampling params\n- Retry policy for transient failures\n- Rate limiting\n\n**Mock Implementation**:\nProvide mock adapter for testing that returns deterministic outputs.","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:12:14.617864492-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:28:23.98688858-07:00","closed_at":"2026-01-15T15:28:23.98688858-07:00","close_reason":"Implemented LLMAdapter ABC with complete, complete_json, count_tokens, count_message_tokens, get_context_limit methods","labels":["adapter","architecture","llm"]}
{"id":"atlas-jwa","title":"Unit tests: Core data models","description":"Write comprehensive unit tests for core data models.\n\n**Coverage Targets**:\n- Artifact model: all fields, validation, serialization\n- WorkItem model: all fields, status transitions\n- Manifest model: all fields, chunk/merge DAG structure\n- Status enum: transition validation\n- WorkType enum: payload schema validation\n\n**Test Cases**:\n- Valid construction with all required fields\n- Optional fields handling\n- Invalid field values (type errors, out of range)\n- JSON/YAML round-trip serialization\n- Immutability where expected\n- Equality comparisons\n- Hash stability for idempotency keys\n\n**Fixtures**:\n- Sample COBOL artifact refs\n- Sample manifests with various sizes\n- Work items in each status\n- Payloads for each work type\n\n**Minimum Coverage**: 95% line coverage for models module","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:12:56.534003203-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:12:53.820196536-07:00","closed_at":"2026-01-15T16:12:53.820196536-07:00","close_reason":"Comprehensive unit tests for core data models. 82 tests covering all model classes.","labels":["testing","unit-test"],"dependencies":[{"issue_id":"atlas-jwa","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:12:56.535651544-07:00","created_by":"andrasfe"},{"issue_id":"atlas-jwa","depends_on_id":"atlas-au2","type":"blocks","created_at":"2026-01-15T15:12:56.537056107-07:00","created_by":"andrasfe"},{"issue_id":"atlas-jwa","depends_on_id":"atlas-dxu","type":"blocks","created_at":"2026-01-15T15:12:56.538618426-07:00","created_by":"andrasfe"},{"issue_id":"atlas-jwa","depends_on_id":"atlas-p00","type":"blocks","created_at":"2026-01-15T15:12:56.539947989-07:00","created_by":"andrasfe"}]}
{"id":"atlas-lpl","title":"Unit tests: COBOL chunk splitter","description":"Write comprehensive unit tests for the COBOL chunk splitter.\n\n**Test Cases**:\n- Simple program: single division per chunk\n- Large PROCEDURE DIVISION: multiple chunks\n- Empty divisions: handle gracefully\n- Nested paragraphs: correct boundaries\n- COPY statements: track references\n- Various COBOL dialects: IBM, MF, etc.\n\n**Determinism Tests**:\n- Same input + profile = identical chunks\n- Chunk IDs are stable across runs\n- Order independence verification\n\n**Boundary Conditions**:\n- Program fits in single chunk\n- Program requires max hierarchy depth\n- Minimum chunk size enforcement\n- Context budget exactly met\n- Context budget exceeded (must split)\n\n**Fixtures**:\n- Sample COBOL files of various sizes\n- Different division structures\n- Edge case programs\n\n**Semantic Correctness**:\n- Chunk boundaries at valid COBOL positions\n- No split mid-statement\n- Paragraph groupings preserved\n\n**Minimum Coverage**: 90% for splitter module","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:13:06.195328774-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:12:53.855341201-07:00","closed_at":"2026-01-15T16:12:53.855341201-07:00","close_reason":"Comprehensive unit tests for COBOL splitter. 80+ tests covering tokenization, boundary detection, edge cases.","labels":["cobol","testing","unit-test"],"dependencies":[{"issue_id":"atlas-lpl","depends_on_id":"atlas-d7c","type":"blocks","created_at":"2026-01-15T15:13:06.19697469-07:00","created_by":"andrasfe"}]}
{"id":"atlas-m98","title":"Implement Phase D: Challenger review handling","description":"Implement Phase D challenger review per spec section 8 Phase D.\n\n**Phase D Flow**:\n1. DOC_CHALLENGE ticket becomes READY after root merge completes\n2. Challenger reads doc_uri + doc_model_uri\n3. Challenger produces Challenge Result artifact with issues + resolution_plan\n4. Challenger marks ticket DONE\n\n**Controller Decision Logic**:\nAfter challenge completes:\n1. Read challenge result artifact\n2. Count issues by severity\n\n**If no issues above threshold**:\n- Create DOC_FINALIZE ticket (optional)\n- Mark job complete\n\n**If issues exist**:\n- Extract resolution_plan\n- Advance to Phase E (follow-up dispatch)\n\n**Configurable Thresholds**:\n- review_policy in manifest defines:\n  - max_iterations\n  - acceptable_severity (ignore MINOR/QUESTION?)\n  - auto_accept_confidence threshold","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:10:42.986875909-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:05.295068584-07:00","closed_at":"2026-01-15T16:35:05.295068584-07:00","close_reason":"Implemented Phase D: Challenger review handling. Enhanced _execute_phase_d to properly handle cycle numbering and use updated documentation paths for cycle \u003e1 challenges. Added _handle_challenge_result method to determine if follow-ups are needed based on BLOCKER/MAJOR issues.","labels":["controller","feature","phase-d"],"dependencies":[{"issue_id":"atlas-m98","depends_on_id":"atlas-1y6","type":"blocks","created_at":"2026-01-15T15:10:42.988613096-07:00","created_by":"andrasfe"}]}
{"id":"atlas-mlw","title":"Define core data models (Artifact, WorkItem, Manifest)","description":"Create Python dataclasses/Pydantic models for the canonical entities defined in spec section 4:\n\n**Artifact (4.1)**:\n- artifact_id (stable logical name)\n- artifact_type (COBOL | COPYBOOK | JCL | OTHER)\n- artifact_version (hash/commit/content hash)\n- artifact_uri (how to fetch content)\n- metadata (optional)\n\n**WorkItem/Ticket (4.2)**:\n- work_id\n- work_type (from section 6)\n- status (from section 5)\n- payload (JSON with artifact/manifest refs)\n- parent_work_id (optional)\n- depends_on (optional list)\n\n**Manifest (4.3)**:\n- job_id\n- artifact_ref\n- analysis_profile\n- splitter_profile\n- context_budget\n- chunks[]\n- merge_dag[]\n- review_policy\n- artifacts (output paths)\n\nInclude validation, serialization to JSON/YAML, and proper type hints.","status":"closed","priority":0,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:07:54.022181181-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:28:21.416272251-07:00","closed_at":"2026-01-15T15:28:21.416272251-07:00","close_reason":"Implemented core data models: Artifact, ArtifactRef, WorkItem with payloads, Manifest with ChunkSpec/MergeNode, and all result artifacts","labels":["architecture","core","models"]}
{"id":"atlas-o1b","title":"Create Python package structure and setup","description":"Set up the Python project structure for the orchestration framework.\n\n**Package Structure**:\n```\natlas/\n  __init__.py\n  models/\n    __init__.py\n    artifact.py\n    work_item.py\n    manifest.py\n    status.py\n    work_types.py\n    outputs.py\n  adapters/\n    __init__.py\n    base.py\n    ticket_system.py\n    artifact_store.py\n    llm.py\n  adapters_impl/\n    __init__.py\n    memory_ticket.py\n    filesystem_store.py\n  controller/\n    __init__.py\n    reconcile.py\n    phases.py\n    routing.py\n  workers/\n    __init__.py\n    scribe.py\n    aggregator.py\n    challenger.py\n    followup.py\n    patch_merge.py\n  splitters/\n    __init__.py\n    base.py\n    cobol.py\n  planners/\n    __init__.py\n    merge_dag.py\n  config/\n    __init__.py\n    settings.py\n  utils/\n    __init__.py\n    idempotency.py\n    observability.py\ntests/\n  unit/\n  integration/\n  fixtures/\n```\n\n**Setup Files**:\n- pyproject.toml\n- setup.cfg or setup.py\n- requirements.txt / requirements-dev.txt\n\n**Dependencies**:\n- pydantic (models)\n- pyyaml (config/manifests)\n- pytest (testing)\n- typing-extensions","status":"closed","priority":0,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:16:14.625288433-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:28:20.64469981-07:00","closed_at":"2026-01-15T15:28:20.64469981-07:00","close_reason":"Created Python package structure with src/atlas layout, pyproject.toml with pydantic/pytest deps, and all module directories","labels":["architecture","infrastructure","setup"]}
{"id":"atlas-p00","title":"Define output artifact schemas","description":"Implement output artifact schemas from spec section 7:\n\n**7.1 Chunk Result Artifact** (DOC_CHUNK output):\n- job_id, artifact_id, artifact_version\n- chunk_id, chunk_locator, chunk_kind\n- summary (narrative)\n- facts (mergeable):\n  - symbols_defined[], symbols_used[]\n  - entrypoints[], paragraphs_defined[]\n  - calls[] (internal/external)\n  - io_operations[]\n  - error_handling[]\n- evidence[] (line ranges/refs)\n- open_questions[]\n- confidence (0..1)\n\n**7.2 Merge Result Artifact** (DOC_MERGE output):\n- job_id, artifact_id, artifact_version, merge_node_id\n- coverage (included inputs, missing/failed)\n- consolidated_facts (call graph, IO map, error handling)\n- conflicts[] (disagreements with follow-up scope)\n- narrative_sections[], doc_fragment_uri\n\n**7.3 Documentation Model Artifact**:\n- doc_uri\n- sections[] with section_id, title, content, source_refs[]\n- index (symbol-\u003echunks, paragraph-\u003echunk, file-\u003echunks)\n\n**7.4 Challenge Result Artifact** (DOC_CHALLENGE output):\n- job_id, artifact_id, artifact_version\n- issues[] with issue_id, severity, question, doc_section_refs, suspected_scopes, routing_hints\n- resolution_plan\n\n**7.5 Follow-up Answer Artifact** (DOC_FOLLOWUP output):\n- issue_id, scope, answer, facts, evidence, confidence","status":"closed","priority":0,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:08:34.468758908-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:31:18.119252082-07:00","closed_at":"2026-01-15T15:31:18.119252082-07:00","close_reason":"Output artifact schemas (ChunkResult, MergeResult, DocumentationModel, ChallengeResult, FollowupAnswer) fully implemented in src/atlas/models/results.py with tests passing","labels":["architecture","core","models"],"dependencies":[{"issue_id":"atlas-p00","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:08:34.47058973-07:00","created_by":"andrasfe"}]}
{"id":"atlas-rt7","title":"Implement in-memory TicketSystem adapter for testing","description":"Create an in-memory implementation of the TicketSystem adapter for unit testing and development.\n\n**Implementation**:\n- Store tickets in a dictionary by work_id\n- Implement all interface methods\n- Support idempotency key tracking\n- Implement basic lease/claim with configurable timeout\n- Thread-safe operations for concurrent tests\n\n**Test Utilities**:\n- reset() method to clear all state\n- seed(tickets) method to pre-populate\n- get_all_transitions(work_id) -\u003e List of status changes\n- assert_ticket_created(work_type, payload_match)\n\n**Status Transition Tracking**:\n- Record all transitions with timestamps\n- Validate transitions match canonical rules\n\nThis adapter will be used extensively in unit tests to verify controller logic without external dependencies.","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:09:06.726448831-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:46:13.983017439-07:00","closed_at":"2026-01-15T15:46:13.983017439-07:00","close_reason":"Implemented MemoryTicketSystem adapter with full support for work item CRUD, status transitions with optimistic locking, claim/release with worker tracking, query operations, idempotency key handling, and comprehensive unit tests (44 tests passing)","labels":["adapter","infrastructure","testing"],"dependencies":[{"issue_id":"atlas-rt7","depends_on_id":"atlas-fu3","type":"blocks","created_at":"2026-01-15T15:09:06.728240801-07:00","created_by":"andrasfe"}]}
{"id":"atlas-udt","title":"Integration test: Ambiguous routing fallback","description":"Create integration test for ambiguous routing per acceptance test scenario 4.\n\n**Scenario** (spec section 15):\nIf challenger can't identify scope, controller creates a bounded cross-cutting follow-up plan rather than one huge task.\n\n**Test Setup**:\n- COBOL program\n- Mock challenger with vague issue (no specific scopes)\n- Routing has no direct matches\n\n**Test Flow**:\n1. Complete initial documentation\n2. Challenger raises issue with no routing hints\n3. Routing algorithm cannot find specific chunks\n4. Controller creates cross-cutting plan:\n   - One follow-up per division/merge-level\n   - Issue-specific merge to consolidate\n5. Follow-ups completed\n6. Consolidation merge produces answer\n7. Patch merge applied\n\n**Assertions**:\n- No single giant follow-up created\n- Multiple bounded follow-ups exist\n- Each follow-up within scope limits\n- Consolidation merge present\n- Issue addressed after patch merge\n- Documentation quality maintained","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:14:46.064700703-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:35:27.407507377-07:00","closed_at":"2026-01-15T16:35:27.407507377-07:00","close_reason":"Created ambiguous routing fallback test verifying issues with invalid chunk IDs fall back to cross-cutting scope handling","labels":["integration-test","routing","testing"],"dependencies":[{"issue_id":"atlas-udt","depends_on_id":"atlas-6mc","type":"blocks","created_at":"2026-01-15T15:14:46.066578224-07:00","created_by":"andrasfe"}]}
{"id":"atlas-vma","title":"Implement observability/logging framework","description":"Implement observability requirements from spec section 13.\n\n**Required Events/Logs**:\n- manifest creation: chunk count, merge levels\n- ticket creation counts by type\n- progress: % chunks complete, % merges complete\n- challenger output: issue counts by severity\n- follow-up dispatch: number of follow-ups per issue\n- patch merges: sections changed, issues addressed\n\n**Interface**:\n```python\nclass ObservabilityEmitter:\n    def emit_manifest_created(job_id, chunk_count, merge_levels)\n    def emit_tickets_created(job_id, counts_by_type: dict)\n    def emit_progress(job_id, phase, completed, total)\n    def emit_challenge_result(job_id, issue_counts: dict)\n    def emit_followup_dispatch(job_id, issue_id, followup_count)\n    def emit_patch_merge(job_id, sections_changed, issues_addressed)\n```\n\n**Storage**:\n- Structured JSON logs\n- Optional metrics emission (abstract)\n- Job summary artifact generation\n\n**Progress Tracking**:\n- Calculate overall job progress\n- Phase-level progress\n- ETA estimation based on historical data","status":"closed","priority":2,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:12:23.894760486-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:03:50.243255858-07:00","closed_at":"2026-01-15T16:03:50.243255858-07:00","close_reason":"Closed","labels":["feature","infrastructure","observability"],"dependencies":[{"issue_id":"atlas-vma","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:12:23.906925525-07:00","created_by":"andrasfe"}]}
{"id":"atlas-yus","title":"Implement idempotency key generation","description":"Create utilities for generating stable idempotency keys per spec section 12.1.\n\n**Key Components**:\n- job_id\n- work_type\n- artifact_version\n- Specific identifier (chunk_id | merge_node_id | issue_id)\n\n**Requirements**:\n- Deterministic: same inputs = same key\n- Collision-free: different work items = different keys\n- Human-readable for debugging\n\n**Format Suggestion**:\n`{job_id}:{work_type}:{artifact_version}:{specific_id}`\n\n**Functions**:\n- generate_chunk_key(job_id, artifact_version, chunk_id) -\u003e str\n- generate_merge_key(job_id, artifact_version, merge_node_id) -\u003e str\n- generate_challenge_key(job_id, artifact_version, cycle) -\u003e str\n- generate_followup_key(job_id, artifact_version, issue_id) -\u003e str\n- parse_idempotency_key(key) -\u003e dict (reverse parsing)\n\n**Manifest Map Support**:\n- If ticket system cannot enforce uniqueness, maintain mapping:\n  idempotency_key -\u003e work_id in artifact store","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:09:55.873867282-07:00","created_by":"andrasfe","updated_at":"2026-01-15T15:36:30.675503126-07:00","closed_at":"2026-01-15T15:36:30.675503126-07:00","close_reason":"Idempotency key generation implemented in src/atlas/utils/hashing.py with compute_work_item_idempotency_key() and compute_work_item_key_from_work_item() functions. Implements spec Section 12.1 requirements: stable keys from job_id, work_type, artifact_version, and type-specific identifiers. Also includes compute_artifact_uri(), compute_result_uri(), and content_hash_short(). 35 comprehensive unit tests in tests/unit/test_hashing.py - all passing","labels":["core","feature","idempotency"],"dependencies":[{"issue_id":"atlas-yus","depends_on_id":"atlas-mlw","type":"blocks","created_at":"2026-01-15T15:09:55.875571647-07:00","created_by":"andrasfe"}]}
{"id":"atlas-z4k","title":"Implement Patch Merge handler","description":"Create the DOC_PATCH_MERGE handler per spec section 6.7.\n\n**Patch Merge Responsibilities**:\n- Apply follow-up answers to existing documentation\n- Update doc_model with new traceability\n- Preserve unchanged sections\n\n**Input**:\n- base_doc_uri + base_doc_model_uri\n- Follow-up answer artifacts (inputs)\n- Output URIs for updated doc\n\n**Processing**:\n1. Load base documentation and doc_model\n2. Load all follow-up answers\n3. For each answer:\n   - Identify affected doc sections using issue routing\n   - Update section content\n   - Update source_refs in doc_model\n4. Write updated doc to output_doc_uri\n5. Write updated doc_model to output_doc_model_uri\n\n**Change Tracking**:\n- Record which sections changed\n- Track which issues were addressed\n- Note any new conflicts discovered\n\n**Output**:\n- Updated documentation artifact\n- Updated doc_model with new traceability\n- Change summary for observability","status":"closed","priority":1,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:12:05.385820294-07:00","created_by":"andrasfe","updated_at":"2026-01-15T16:12:53.786023923-07:00","closed_at":"2026-01-15T16:12:53.786023923-07:00","close_reason":"Patch Merge handler implemented with comprehensive aggregation logic.","labels":["feature","patch-merge","worker"],"dependencies":[{"issue_id":"atlas-z4k","depends_on_id":"atlas-apq","type":"blocks","created_at":"2026-01-15T15:12:05.387696241-07:00","created_by":"andrasfe"},{"issue_id":"atlas-z4k","depends_on_id":"atlas-6b8","type":"blocks","created_at":"2026-01-15T15:12:05.389155206-07:00","created_by":"andrasfe"}]}
{"id":"atlas-zoq","title":"Implement DOC_FINALIZE handler","description":"Implement the optional DOC_FINALIZE work type per spec section 6.8.\n\n**Purpose**:\nProduce final deliverables and mark job accepted.\n\n**Final Deliverables**:\n- Markdown documentation (human-readable)\n- PDF generation (optional)\n- Trace report (chunk to doc section mapping)\n- Summary statistics\n\n**Trace Report Contents**:\n- Per-section: which chunks contributed\n- Per-chunk: confidence, open questions resolved\n- Challenger iterations history\n- Issues raised and resolved\n\n**Job Completion**:\n- Mark DOC_REQUEST as fully complete\n- Final status summary\n- Cleanup any temporary artifacts (optional)\n\n**Configuration**:\n- Which outputs to generate\n- Output format preferences\n- Cleanup policy","status":"closed","priority":3,"issue_type":"task","owner":"andrasf94@gmail.com","created_at":"2026-01-15T15:15:53.874803453-07:00","created_by":"andrasfe","updated_at":"2026-01-15T17:30:29.105684-07:00","closed_at":"2026-01-15T17:30:29.105684-07:00","close_reason":"Closed","labels":["feature","finalize","worker"],"dependencies":[{"issue_id":"atlas-zoq","depends_on_id":"atlas-4ph","type":"blocks","created_at":"2026-01-15T15:15:53.876607255-07:00","created_by":"andrasfe"}]}
